# The story of Chainalytic

1. Why was it born ?
2. Architecture design choice
3. Potential values created by Chainalytic
4. Future plan

First, please take a look at link:README.adoc[README] to know what Chainalytic is able to do if you haven't yet.

## Why was it born ?

Right after the decentralization of ICON mainnet, my team - ICONVIET - was starting to build a more advanced explorer/tracker for ICON public chain. People of ICON deserve a better chain explorer. However, although the official JSON-RPC API is good enough for basic needs, it is lacking of many fine-grained information and data, which would be very helpful to ICONists and dev teams need to interact with data on the chain.

Let's dig a bit deeper into what exactly derived data is. Basically, there are 2 mains categories

- Historical time series data
 * For instance, the total amount of staked ICX at some specific block height, for all blocks from starting block to latest one
- Chain state data
 * For instance, the list of wallets currently in unlocking period ( unstaking )

Both are updated after each block as the result, via the execution of on-chain `transactions` ( also known as `transition functions` ). Hence, depend on your own need, historical and state data would variate a lot. Apparently, they need to be derived/generated differently for each specific requirement.

Even if the official API provides more data and metric, it simply cannot serve every demand. The amount of derivable data is almost infinite. Individual ICONists and dev teams probably need specific and unique kind of data derived from on-chain state transitions over time. With all those facts, baking all derived data into the chain database is unrealistic and potentially making the chain unnecessarily huge.

So, what should we do to get more data ? There was a few approaches

- Using official API, we query current latest block TX data, then calculate, generate and store new data on-the-fly. The issue is, it only works for...future blocks. For old blocks we missed, they are gone forever.

- Using official API, we query TX data from the very first blocks and do all transaction executions from scratch. It is somewhat like re-executing the chain from beginning, the most canonical approach. However, feeding data via official RPC API is very slow. And for every new kind of data we desire, we need to re-implement data aggregation logic. As we see, this approach is not so efficient and scalable.

- Same approach as above one, re-execution of transactions. However we push it further to some form of "framework" or "engine". In that way, we aim to build a framework which is designed to be easily customized and extended, good performance, as well as come with many choices in upstream/source data feeding method. Ultimately, it must be efficient, scalable, and easy for developer to add functionality. Yeah, *Chainalytic* was born to satisfy them all.

## Architecture design choice

In favor of *modularity*, *extensibility* and *chain-agnostic*, I decided to make Chainalytic a multi-process/multi-service application which is also an extensible framework itself.

Basically, it has 4 components ( 4 services ), which can run in the same machine/host or in different machines/hosts. These services talk to each other via *JSON-RPC API* running on top of *Asynchronous WebSocket* as underlying transfer protocol.

- Upstream
- Aggregator
- Warehouse
- Provider

For each working session, these services run with one pre-configured identity called `zone_id` that represent for a specific chain. For instance, `public-icon` is a Chainalytic built-in zone implemented for ICON mainnet.

Please look link:docs/resource/chainalytic.png[here] for illustration of relationship between these services.

### Upstream

In charge of feeding original on-chain transaction data to `Aggregator`. 

It offers 2 choices of data feeding from blockchain node client software

- Read data directly from client database for optimum performance ( `LevelDB` in case of ICON ). Due to LevelDB restriction of single access instance, client ( ICON citizen node ) must be shutdown before feeding data. +
Should use this for new data aggregation from scratch.
- Request for data via RPC calls to client using official API. In this way client can keep running, and data is almost fed in realtime with client. +
Should use this if data is already fully aggregated recently.

### Aggregator

Constantly ask for new block TX data from `Upstream` and handle data derivation/aggregation for each new block, then sending all new data to `Warehouse` for storage.

Each kind of data derivation logic is implemented in form of `Transform` which is independent of each other and has its own database, known as *cache* database, e.g. *"stake_history_cache"*

For zone `public-icon`, Chainalytic is shipped with 4 built-in `Transforms`, see link:docs/provider_api.adoc[API docs here] to know what they offer

- stake_history
- stake_top100
- recent_stake_wallets
- abstention_stake

### Warehouse

It receives and stores data from `Aggregator` as well as provide stored data to `Provider` and `Aggregator`.

Databases are organized by `Transforms`. It is called *storage* database, e.g *"stake_history_storage"*

At this point, these databases are also implemented in LevelDB for convenience and performance purpose.

### Provider

The name says it all, it provide data to external consumers ( any application ) via *JSON-RPC* on top of *Asynchronous aiohttp* transfering layer.

The way data is exposed to external world is defined by `ApiBundle` which asks for `Collator` to request data from `Warehouse`

## Potential values created by Chainalytic

You know, data, if analyzed properly could bring valuable insights, especially with the rising tide of data science, machine learning and AI.

When I started to work on Chainalytic, I wondered if it really worth the effort at all. In the end, I believed the answer would be *yes*, we could see it more clearly if ICON network growth exponentially in coming years, there would be a lot of data need to be generated and analyzed.

For now, it already helped ICONVIET to build link:https://iconlook.io[ICONLOOK] which is a chain explorer with many unique features like staking, unstaking and unlocking metrics. Those metrics could be a good measure of community sentiment and health of network.

## Future plan

There is a lot of things need to be done, like Chainalytic UI, more interesting `Transforms`, as well as meaningful applications which utilize data generated by Chainalytic and bring economic/utility value.

Honestly, at this point, I am still working on long term plan for Chainalytic, it is not clear yet. 

That said, for short term, I am going to build *Chainalytic UI*.